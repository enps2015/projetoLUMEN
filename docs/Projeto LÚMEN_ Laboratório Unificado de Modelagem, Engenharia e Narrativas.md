**Projeto LÃšMEN: LaboratÃ³rio Unificado de Modelagem, Engenharia e Narrativas
VersÃ£o:** 1.0 - "A Centelha da GÃªnese"
**Autor:** Eric Pimentel
**Data de CriaÃ§Ã£o:** 24 de junho de 2025
**_"Onde hÃ¡ dados, hÃ¡ padrÃ£o. Onde hÃ¡ padrÃ£o, hÃ¡ informaÃ§Ã£o. Onde
hÃ¡ informaÃ§Ã£o, hÃ¡ narrativa. E onde hÃ¡ narrativa, hÃ¡ o poder de
transformar a realidade. LÃšMEN Ã© a nossa forja de narrativas."_**
**Resumo Executivo (A VisÃ£o em Um Olhar)**
O Projeto LÃšMEN Ã© um ecossistema de dados ponta-a-ponta, totalmente
open-source e containerizado, projetado para ser uma plataforma robusta, escalÃ¡vel
e reprodutÃ­vel para anÃ¡lise de dados, engenharia de dados e experimentaÃ§Ã£o com
Machine Learning. Ele nasce da filosofia de que ferramentas de nÃ­vel profissional
nÃ£o devem ser um privilÃ©gio corporativo, mas um poder acessÃ­vel a todos os
estudiosos e criadores.
Esta arquitetura integra Python, PostgreSQL, Metabase, Mage.ai e MLflow,
orquestrados por Docker, para criar um ambiente que simula uma stack de dados
moderna, ideal para portfÃ³lio, estudos avanÃ§ados e desenvolvimento de projetos de
impacto.

**1. VisÃ£o e MissÃ£o (A Estrela Guia)**
    â— ğŸŒŒ **VisÃ£o:** Ser um ecossistema de referÃªncia, modular e evolutivo, capaz de
       transformar dados brutos de qualquer fonte em sabedoria acionÃ¡vel, desde
       dashboards interativos a modelos preditivos inteligentes.
    â— ğŸ¯ **MissÃ£o:** Construir e documentar um MVP (Minimum Viable Product) da
       plataforma LÃšMEN, demonstrando um fluxo de dados completo: ingestÃ£o
       automatizada, armazenamento estruturado, transformaÃ§Ã£o alquÃ­mica e
       visualizaÃ§Ã£o eloquente.
    â— **O5: [Ambiente de AnÃ¡lise ReprodutÃ­vel]** Disponibilizar um ambiente de
       anÃ¡lise interativo (JupyterLab) integrado Ã  stack, permitindo a exploraÃ§Ã£o
       de dados e o desenvolvimento de modelos de forma coesa e versionada.


**3. Arquitetura da ConstelaÃ§Ã£o LÃšMEN (O Mapa Estelar)**
A arquitetura Ã© baseada em serviÃ§os desacoplados e especializados,
comunicando-se atravÃ©s de uma rede Docker interna.

**Os Pilares da ConstelaÃ§Ã£o:**
*   **PostgreSQL (O Templo do Saber):** O guardiÃ£o de todo o conhecimento.
*   **Metabase (O ObservatÃ³rio CÃ³smico):** A janela para a alma dos dados.
*   **Mage.ai (Os Portais de Coleta):** O orquestrador dos fluxos de dados.
*   **JupyterLab (O LaboratÃ³rio do Alquimista):** O santuÃ¡rio para a exploraÃ§Ã£o,
    anÃ¡lise e ciÃªncia de dados interativa.


**Fluxo de Dados LÃ³gico do MVP:**
[Fonte Externa: API do INMET]
|
v
[1. Mage.ai (Container)] --(Aciona o Pipeline Agendado)-->
| 1.1 Extrai os dados
|
v
[2. PostgreSQL (Container)] --(Armazena em schema `raw_data`)-->
|
v
[3. Script Python (Invocado pelo Mage)] --(LÃª de `raw_data`)-->
| 3.1 Limpa, transforma, enriquece
|
v
[2. PostgreSQL (Container)] --(Salva em schema `processed_data`)-->
|
v
[4. Metabase (Container)] --(UsuÃ¡rio acessa o Dashboard)-->
| 4.1 Consulta `processed_data`
|
v
[Navegador do UsuÃ¡rio Final] --(Visualiza a narrativa de dados)-->

**Fluxo de AnÃ¡lise e ExploraÃ§Ã£o:**
[Cientista de Dados no JupyterLab (Container)] <--> [2. PostgreSQL (Container)]
          (LÃª de `raw_data` e `processed_data` para anÃ¡lises)


**4. Plano de AÃ§Ã£o para o MVP (A Jornada do Construtor)**
Dividiremos nossa jornada em fases claras e sequenciais.
    â— **Fase 0: GÃªnese (DuraÃ§Ã£o: 2 horas)**
       â—‹ [ ] Criar um repositÃ³rio Git (no GitHub, GitLab, etc.).
       â—‹ [ ] Criar a estrutura inicial de pastas (/data, /notebooks, /scripts).
       â—‹ [ ] Escrever a primeira versÃ£o do docker-compose.yml com os serviÃ§os


```
postgres e metabase.
â—‹ [ ] Criar o arquivo Makefile para os comandos de automaÃ§Ã£o.
â— Fase 1: O Templo (DuraÃ§Ã£o: 3 horas)
â—‹ [ ] Iniciar o serviÃ§o do PostgreSQL via Docker.
â—‹ [ ] Conectar-se ao banco usando uma ferramenta como DBeaver ou psql e
criar os schemas: raw_data, processed_data, analytics.
â—‹ [ ] Validar a persistÃªncia dos dados reiniciando o contÃªiner.
â— Fase 2: O Ritual da IngestÃ£o (DuraÃ§Ã£o: 4 horas)
â—‹ [ ] Adicionar o serviÃ§o mage ao docker-compose.yml.
â—‹ [ ] Criar um novo projeto Mage e desenvolver o pipeline de extraÃ§Ã£o da API
do INMET.
â—‹ [ ] Testar o bloco de "data loader" para garantir que os dados brutos sÃ£o
salvos na tabela correta no schema raw_data.
â— Fase 3: A Alquimia (DuraÃ§Ã£o: 4 horas)
â—‹ [ ] Desenvolver o script Python de transformaÃ§Ã£o em um bloco "transformer"
no Mage.
â—‹ [ ] O script deve ler da raw_data, tratar valores nulos, converter tipos de
dados e salvar o resultado em processed_data.
â—‹ [ ] Adicionar testes de qualidade de dados bÃ¡sicos no script (ex: checar se a
temperatura estÃ¡ em uma faixa plausÃ­vel).
â— Fase 4: O OrÃ¡culo (DuraÃ§Ã£o: 3 horas)
â—‹ [ ] Configurar a conexÃ£o do Metabase com o nosso PostgreSQL.
â—‹ [ ] Criar as "perguntas" (queries) no Metabase apontando para a tabela
processada.
â—‹ [ ] Montar o dashboard final do MVP.
    â— **Fase 5: O LaboratÃ³rio (DuraÃ§Ã£o: 2 horas)**
       â—‹ [ ] Adicionar o serviÃ§o do JupyterLab ao docker-compose.yml, mapeando
         a pasta /notebooks.
       â—‹ [ ] Garantir que o Jupyter consiga se conectar ao PostgreSQL.
       â—‹ [ ] Criar um notebook de exemplo que lÃª dados do banco.

**5. O "Algo a Mais" (Inspirando ConfianÃ§a e Profissionalismo)**
Para elevar nosso MVP de "funcional" para "impressionante", incluiremos:
    â— **Makefile de AutomaÃ§Ã£o:** Um Makefile na raiz do projeto com alvos simples e
       poderosos:
          â—‹ make up: Inicia toda a stack com docker-compose up.
          â—‹ make down: Para todos os serviÃ§os.


```
â—‹ make logs: Exibe os logs de todos os serviÃ§os.
â—‹ make clean: Remove volumes e redes para um reinÃ­cio limpo.
â— Testes de Qualidade de Dados (Data Quality Checks): Dentro do nosso script
Python de transformaÃ§Ã£o, implementaremos funÃ§Ãµes explÃ­citas que validam os
dados. Se um dado invÃ¡lido for encontrado (ex: umidade acima de 100%), o
pipeline deve falhar e registrar um erro claro. Isso demonstra uma mentalidade
de engenharia robusta.
â— VariÃ¡veis de Ambiente: NÃ£o haverÃ¡ senhas ou chaves de API "hardcoded".
Usaremos um arquivo .env (incluÃ­do no .gitignore) para gerenciar as credenciais,
que serÃ£o lidas pelo docker-compose.yml.
```
**6. PrÃ³ximos Passos (AlÃ©m do Horizonte do MVP)**
Uma vez que o LÃšMEN esteja estÃ¡vel, a jornada continua:
    â— **IntegraÃ§Ã£o do MLflow:** Adicionar o serviÃ§o do MLflow para comeÃ§ar a registrar
       experimentos de Machine Learning.
    â— **Pipeline de Treinamento:** Criar um novo pipeline no Mage para treinar um
       modelo simples (ex: prever a temperatura da prÃ³xima hora) e logÃ¡-lo no MLflow.
    â— **IntegraÃ§Ã£o com dbt:** Para transformaÃ§Ãµes mais complexas e testes de dados
       mais avanÃ§ados, o dbt pode ser integrado como a principal ferramenta de
       transformaÃ§Ã£o.
    â— **Alerta:** Configurar alertas no Metabase ou via uma ferramenta externa quando
       certos limiares nos dados forem atingidos.


